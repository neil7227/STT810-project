\documentclass[12pt]{article}

\usepackage[margin=1in]{geometry}
\usepackage{amsmath, amssymb}
\usepackage{graphicx}
\usepackage{booktabs}
\usepackage{hyperref}
\usepackage{setspace}
\usepackage{caption}
\usepackage{float}
\usepackage{array}

\hypersetup{
  colorlinks=true,
  linkcolor=blue,
  urlcolor=blue,
  citecolor=blue
}

\doublespacing

\title{STT 810 Project:\\
Predicting CPBL Hitter Performance}
\author{Wei-Chieh, Tseng: 181608938 \and Draco Hong: 181591665 \and Cheng-Lun Lee: 181487980}
\date{\today}

\begin{document}
\maketitle
\newpage

\begin{abstract}
This project looks at which offensive statistics matter most for hitters in the Chinese Professional Baseball League (CPBL).
We use OPS+ as our main measure of hitting performance and compare three types of models.

First, we treat OPS+ as a continuous outcome and fit robust regression (Huber regression), with and without principal component analysis (PCA), to deal with outliers and correlated predictors.

Second, we define a binary ``elite’’ indicator based on OPS+ and fit logistic regression models using plate discipline and batted-ball statistics such as strikeout rate, BABIP, and balls-in-play rate. With four process-based predictors (PA, K\_pct, BABIP, BIP\_pct), the logistic model reaches about 87\% test accuracy and a test AUC around 0.93 on a hold-out set, and about 89\% accuracy on the full sample. These results suggest that a low strikeout rate and a high BABIP are strong signals of elite CPBL hitters.

Third, to avoid using statistics that are already built into OPS+, we fit a Ridge regression model that predicts OPS+ directly from eight standardized process-based features (PA, AVG, BABIP, BIP\_pct, BB\_pct, BB/K, K\_pct, PutAway\_pct), and we leave out OBP, SLG, wOBA, RC, and ISO. Using an 80/20 train–test split with 5-fold cross-validation on the training set, this Ridge model attains a test-set $R^2 \approx 0.97$, which shows that these process-based measures alone can predict OPS+ very well.

Overall, the robust regression, logistic regression, and Ridge regression models all point to the same main factors: avoiding strikeouts, making good contact (BABIP, BIP\_pct), and creating runs are key drivers of OPS+ for CPBL hitters.
\end{abstract}

\newpage

\tableofcontents
\newpage

\section{Introduction}

OPS+ is a common baseball statistic that combines a hitter's on-base
and slugging performance,  for balance the different leagues and parks effects.
In this project, we focus on batters from the Chinese Professional
Baseball League (CPBL) and look at two questions:
\begin{enumerate}
  \item Which offensive statistics have the strongest relationship with OPS+?
  \item Can we build reasonably simple models that use process-based
        statistics (like plate discipline and batted-ball rates) to
        predict or classify hitter performance?
\end{enumerate}

We use CPBL data from the 2024 and 2025 seasons and try three kinds of models.
First, we treat OPS+ as a continuous outcome and fit robust regression
models (Huber regression), with and without principal component analysis
(PCA), to deal with outliers and correlated variables.
Second, we turn OPS+ into a binary ``elite'' indicator and fit logistic
regression models based on plate discipline and batted-ball variables.
Third, we fit a Ridge regression model that predicts OPS+ from a set of
independent process-based features.

Our code and cleaned data are stored in a public GitHub repository:
\begin{center}
  \href{https://github.com/neil7227/STT810-project/tree/main}{\texttt{https://github.com/neil7227/STT810-project/tree/main}}
\end{center}

\section{Data and Preprocessing}

\subsection{Data source}

We analysis CPBL batter data from the 2024 and 2025 seasons, and the data is comes from a
public baseball statistics website (Robas, \textit{The Baseball Revolution}).
The raw dataset has basic batting statistics for 307 players and 18
columns, including:
\begin{itemize}
  \item Plate appearances (PA), at-bats, hits, doubles, triples, home runs.
  \item Walks, strikeouts, hit-by-pitch, sacrifice flies.
  \item Summary rate statistics such as batting average (AVG),
        on-base percentage (OBP), slugging percentage (SLG), OPS, and OPS+.
\end{itemize}

\subsection{Cleaning and feature construction}

We use the following preprocessing steps (these are also recorded in our
notebooks and cleaned CSV files in the GitHub repo):
\begin{enumerate}
  \item Remove duplicate or clearly redundant columns such as OPS (since OPS+
        is already in the data) and some other highly collinear summary stats.
  \item Make sure numeric variables are stored as numeric types and
        fix obvious missing or inconsistent entries.
  \item Drop players with very few plate appearances to avoid huge
        noise from tiny samples.
        For the logistic regression analysis we keep hitters with at
        least 50 PA, which leaves about 100 players.
  \item Build process-based variables, for example:
        \begin{itemize}
          \item Walk rate: $\text{BB\_pct} = \frac{\text{BB}}{\text{PA}}$.
          \item Strikeout rate: $\text{K\_pct} = \frac{\text{SO}}{\text{PA}}$.
          \item Balls-in-play rate: BIP\_pct.
          \item Batting average on balls in play (BABIP).
          \item Put-away rate in two-strike counts (PutAway\_pct).
        \end{itemize}
  \item Keep OPS+ as the main performance variable we care about.
\end{enumerate}

\begin{table}[H]
\centering
\caption{Key variables used in the analysis.}
\label{tab:variables}
\begin{tabular}{>{\raggedright}p{0.25\textwidth} p{0.65\textwidth}}
\toprule
Variable & Description \\
\midrule
OPS\_plus    & League- and park-adjusted OPS index (100 = league average) \\
PA           & Plate appearances \\
BB\_pct      & Walk rate (BB / PA) \\
K\_pct       & Strikeout rate (SO / PA) \\
BABIP        & Batting average on balls in play \\
BIP\_pct     & Balls-in-play rate \\
PutAway\_pct & Two-strike put-away rate \\
SLG          & Slugging percentage \\
\bottomrule
\end{tabular}
\end{table}

\subsection{Exploratory analysis}

\begin{figure}[H]
  \centering
  \includegraphics[width=0.9\textwidth]{word_fig01_data_head.png}
  \caption{Head of the CPBL batter dataset (example rows).}
  \label{fig:data_head}
\end{figure}

We then keep only numeric columns and compute the correlation matrix.
From the logistic-regression notebook we get the following correlation
heatmap:

\begin{figure}[H]
  \centering
  \includegraphics[width=0.9\textwidth]{logit_corr_heatmap.png}
  \caption{Correlation heatmap of numeric features in the CPBL dataset.}
  \label{fig:correlation}
\end{figure}

The heatmap shows that slugging percentage (SLG) and several batted-ball
variables have a strong relationship with OPS+.
This is why we include them in the robust regression model.
For the logistic regression model, we use process-based plate
discipline and batted-ball rates instead of directly using OPS+ or SLG
as predictors.

\section{Methods}
\subsection{Robust regression for continuous OPS+}
\subsubsection{Model choice}
Baseball data often contain outliers, for example players with very few
plate appearances or one-year spikes that look very different from the rest.
Ordinary least squares (OLS) regression can be quite sensitive to these
extreme points, so we use Huber regression to predict OPS+ from a set of
offensive statistics and compare models based on the original features
and on PCA components.
Let $y_i$ be the OPS+ of player $i$ and $\mathbf{x}_i$ be a vector of
predictors (for example OBP, SLG, BB\_pct, K\_pct).
The Huber regression solves
\[
  \min_{\beta_0, \boldsymbol{\beta}} \sum_{i=1}^n \rho_\delta
  \bigl(y_i - \beta_0 - \mathbf{x}_i^\top \boldsymbol{\beta}\bigr),
\]
where $\rho_\delta(\cdot)$ is the Huber loss with tuning parameter
$\delta$: it behaves like squared loss for small residuals and like absolute
loss for large residuals.

\subsubsection{PCA-based dimension reduction}
We first standardize the main batting statistics and apply PCA to them.
Instead of only looking at the variance explained by each component, we
fit a sequence of Huber regression models using the first
$k = 1,\dots,6$ principal components and record several model
measures (RMSE, $R^2$, adjusted $R^2$, and AIC).
This lets us see how prediction accuracy changes as we change
the number of components we keep.

\begin{figure}[H]
  \centering
  \includegraphics[width=0.9\textwidth]{word_fig04_pca_model_metrics.png}
  \caption{Model measures (RMSE, $R^2$, adjusted $R^2$, and AIC) for
  Huber regression models using the first 1--6 principal components.}
  \label{fig:pca_model_metrics}
\end{figure}

Figure~\ref{fig:pca_model_metrics} shows that RMSE and AIC drop a lot
when we move from one to two components and keep improving up to about
five components, while $R^2$ and adjusted $R^2$ quickly get close to 0.99.
Based on this plot, we keep the first five principal components in the
PCA-based Huber model.

\subsection{Logistic regression for elite vs.\ non-elite hitters}
\subsubsection{Outcome and predictors}
For logistic regression we define a binary outcome:
\[
  \text{elite}_i =
  \begin{cases}
    1, & \text{if } \text{OPS+}_i \geq 110, \\
    0, & \text{if } \text{OPS+}_i < 110.
  \end{cases}
\]
We keep hitters with at least 50 PA (about 100 players).
We use process-based predictors from \texttt{cpbl\_logistic.ipynb}:
\begin{itemize}
  \item PA
  \item BB\_pct
  \item K\_pct
  \item BABIP
  \item BIP\_pct
  \item PutAway\_pct
\end{itemize}
All predictors are standardized before modeling.

\subsubsection{Two implementations}
We use two implementations:
\begin{enumerate}
  \item \textbf{scikit-learn} \texttt{LogisticRegression} with a 70/30
        train--test split to measure out-of-sample performance.
  \item \textbf{statsmodels} \texttt{Logit} on the full sample, first with
        all six predictors and then a reduced model with
        \[
          \{\text{PA}, \text{K\_pct}, \text{BABIP}, \text{BIP\_pct}\}
        \]
        chosen based on $p$-values.
\end{enumerate}
We report training and test accuracy, confusion matrices, simple
classification summaries, and ROC/AUC.

\subsection{Ridge regression with independent features}
\subsubsection{Model choice}
To avoid predicting OPS+ from statistics that are almost built into its
formula, we fit a Ridge regression model using only
process-based predictors.
Ridge regression adds an $L_2$ penalty to the coefficient estimates,
which helps avoid overfitting and deals with multicollinearity among
predictors.
Let $y_i$ be the OPS+ of player $i$ and $\mathbf{x}_i$ a vector of
independent features:
\[
  \mathbf{x}_i = (\text{PA}, \text{AVG}, \text{BABIP}, \text{BIP\_pct},
  \text{BB\_pct}, \text{BB/K}, \text{K\_pct}, \text{PutAway\_pct})^\top.
\]
The Ridge regression solves
\[
  \min_{\beta_0, \boldsymbol{\beta}} \sum_{i=1}^n
  \bigl(y_i - \beta_0 - \mathbf{x}_i^\top \boldsymbol{\beta}\bigr)^2
  + \alpha \|\boldsymbol{\beta}\|_2^2,
\]
where $\alpha > 0$ is a penalty parameter that shrinks the coefficients
toward zero.
We standardize all predictors before modeling and use $\alpha = 1.0$.

\subsubsection{Feature selection and validation}
We leave out OBP, SLG, wOBA, RC, and ISO from the predictor set because
these statistics are components or simple transformations of OPS+.
By using only the eight independent features above, we focus on how well
we can predict OPS+ from process metrics
(contact rate, walk rate, strikeout rate, etc.) instead of from outcome
measures.
We split the data into 80\% training and 20\% test sets (random seed 42)
and evaluate the model with 5-fold cross-validation on the training set.
We look at RMSE, MAE, and $R^2$ on both the training and test sets.
The test-set $R^2$ is about 0.97, so in this sample
process-based statistics alone already explain most of the variation in OPS+.

\section{Results}

\subsection{Robust regression results}

\subsubsection{Original data vs.\ PCA representation}

\begin{figure}[H]
  \centering
  \includegraphics[width=0.85\textwidth]{word_fig02_origin_plot.png}
  \caption{Robust regression using original features (example fit).}
  \label{fig:origin_robust}
\end{figure}

\begin{figure}[H]
  \centering
  \includegraphics[width=0.85\textwidth]{word_fig05_pca_huber.png}
  \caption{Robust regression using PCA (95\% variance) features.}
  \label{fig:pca_robust}
\end{figure}

The PCA-based model has similar or slightly better performance than the model
that uses the original features.
It also uses fewer predictors and produces residuals that look more stable.

\subsubsection{Effect of log transformation}

We also tried a log transformation of OPS+ for robust
regression.
The log-transformed model fit the data worse and did not make the
results easier to explain, so we keep OPS+ on its original scale.

\begin{figure}[H]
  \centering
  \includegraphics[width=0.8\textwidth]{word_fig06_log_plot.png}
  \caption{Effect of log-transforming OPS+ in robust regression.}
  \label{fig:log_transform}
\end{figure}

\subsubsection{Comparison and feature importance}

\begin{figure}[H]
  \centering
  \includegraphics[width=0.8\textwidth]{word_fig07_compare_models.png}
  \caption{Comparison of robust regression models (original vs.\ PCA vs.\ log).}
  \label{fig:robust_compare}
\end{figure}

\begin{figure}[H]
  \centering
  \includegraphics[width=0.4\textwidth]{word_fig08_feature_weights.png}
  \caption{Estimated feature weights from the robust regression model.}
  \label{fig:robust_features}
\end{figure}

The feature-weight plot shows that slugging percentage (SLG) is one
of the most important predictors of OPS+, which matches baseball intuition:
extra-base hits and power have a big impact on OPS+.

\subsection{Logistic regression results}

\subsubsection{Model building and variable selection}

We start from a full logistic regression model with all six process-based
predictors:
\[
  \{\text{PA}, \text{BB\_pct}, \text{K\_pct}, \text{BABIP},
    \text{BIP\_pct}, \text{PutAway\_pct}\}.
\]
Using \texttt{statsmodels} \texttt{Logit} with standardized predictors and an
intercept, the full model reaches a pseudo-$R^2$ of about 0.64 and the
likelihood ratio test clearly rejects the null of no covariate effects.
The coefficient table in Figure~\ref{fig:logit_full_summary} shows that PA,
K\_pct, BABIP and BIP\_pct are important, while BB\_pct and PutAway\_pct have
much larger $p$-values (around 0.17 and 0.74).

\begin{figure}[H]
  \centering
  \includegraphics[width=0.9\textwidth]{logit_full_model_summary.png}
  \caption{Summary output for the full logit model with six process-based predictors.}
  \label{fig:logit_full_summary}
\end{figure}

Based on these $p$-values, we fit a reduced logit model that keeps only
\[
  \{\text{PA}, \text{K\_pct}, \text{BABIP}, \text{BIP\_pct}\}.
\]
The reduced model has a pseudo-$R^2$ of about 0.61, slightly lower than the
full model but still high, and it has a similar log-likelihood with fewer
parameters.
Dropping BB\_pct and PutAway\_pct does not noticeably hurt the fit but makes
the model simpler and easier to explain.
We use this four-variable model for both the train--test
evaluation and the final discussion; its summary output is shown in
Figure~\ref{fig:logit_reduced_summary}.

\begin{figure}[H]
  \centering
  \includegraphics[width=0.9\textwidth]{logit_reduced_model_summary.png}
  \caption{Summary output for the reduced logit model with four predictors
  (PA, K\_pct, BABIP, BIP\_pct).}
  \label{fig:logit_reduced_summary}
\end{figure}

\subsubsection{Train--test performance (scikit-learn, 4 features)}

Using the reduced feature set $\{\text{PA}, \text{K\_pct}, \text{BABIP},
\text{BIP\_pct}\}$ and a 70/30 train--test split, we fit a
\texttt{LogisticRegression} model in \texttt{scikit-learn} with
\texttt{max\_iter = 1000} and \texttt{class\_weight = "balanced"}.
This model gives:

\begin{itemize}
  \item Training accuracy: about 0.83.
  \item Test accuracy: about 0.87.
  \item Test AUC: around 0.93.
\end{itemize}

On the 30\% test set, the confusion matrix (rows = true class, columns
= predicted class) from the notebook is:

\begin{table}[H]
\centering
\caption{Test-set confusion matrix (scikit-learn logistic regression, 4 features).}
\label{tab:cm_test}
\begin{tabular}{c|cc}
\toprule
 & Predicted 0 & Predicted 1 \\
\midrule
True 0 & 19 & 2 \\
True 1 & 2  & 7 \\
\bottomrule
\end{tabular}
\end{table}

So on the test set, the model correctly classifies most non-elite hitters and most
elite hitters.
The ROC curve on the test set is shown in Figure~\ref{fig:logit_roc_test}.

\begin{figure}[H]
  \centering
  \includegraphics[width=0.8\textwidth]{logit_roc_test_4feat.png}
  \caption{ROC curve for the reduced logistic regression model on the test set.}
  \label{fig:logit_roc_test}
\end{figure}

\subsubsection{Full-sample inference (statsmodels reduced logit)}

For more detailed inference, we use \texttt{statsmodels} again and fit the
reduced logit model with the same four predictors on the full sample of about
100 hitters.
Using a probability threshold of 0.5 to classify elite vs.\ non-elite, the
confusion matrix is:

\begin{table}[H]
\centering
\caption{Full-sample confusion matrix (statsmodels reduced logit).}
\label{tab:cm_full}
\begin{tabular}{c|cc}
\toprule
 & Predicted 0 & Predicted 1 \\
\midrule
True 0 & 66 & 5 \\
True 1 & 6  & 23 \\
\bottomrule
\end{tabular}
\end{table}

The in-sample accuracy is close to 0.89.
The ROC curve in Figure~\ref{fig:logit_roc_full} shows that the in-sample
AUC is also above 0.9, so the model separates elite and non-elite hitters
quite well on the full data.

\begin{figure}[H]
  \centering
  \includegraphics[width=0.8\textwidth]{logit_roc_full_reduced_logit.png}
  \caption{ROC curve for the reduced statsmodels logit on the full sample.}
  \label{fig:logit_roc_full}
\end{figure}

We also turn the estimated coefficients into odds ratios via
\[
\text{OR}_j = \exp(\hat{\beta}_j),
\]
and explain them on the standardized scale:
\begin{itemize}
  \item A one-standard-deviation increase in PA increases the odds of
        being elite (odds ratio larger than 1).
  \item A one-standard-deviation increase in K\_pct clearly lowers
        the odds.
  \item A one-standard-deviation increase in BABIP increases the odds
        by a large factor.
  \item Higher BIP\_pct is linked to lower odds of being elite.
\end{itemize}

\subsection{Ridge regression results}

\subsubsection{Model performance and validation}

The Ridge regression model with eight independent features gives strong
prediction on both the training and test sets.
With an 80/20 train--test split and 5-fold cross-validation on the training
set, the model gives:

\begin{itemize}
  \item Training $R^2$: 0.972
  \item Test $R^2$: 0.968
  \item Training RMSE: 5.88
  \item Test RMSE: 6.30
  \item Training MAE: 4.48
  \item Test MAE: 4.82
  \item Cross-validation $R^2$ (mean): 0.965
\end{itemize}

The test-set $R^2$ of 0.968 means that, in this dataset, process-based
statistics alone can predict OPS+ with high accuracy, even without using
outcome measures that are directly part of the OPS+ formula.

\begin{figure}[H]
  \centering
  \includegraphics[width=0.9\textwidth]{ridge_regression_independent.png}
  \caption{Ridge regression diagnostic plots using independent features:
  predicted vs.\ actual OPS+ (top left), residuals vs.\ predicted values
  (top right), top 10 feature coefficients (bottom left), and distribution
  comparison (bottom right).}
  \label{fig:ridge_results}
\end{figure}

Figure~\ref{fig:ridge_results} shows the model checks.
The predicted vs.\ actual plot (top left) has points close to
the diagonal line, so the predictions are close to the observed OPS+ values.
The residual plot (top right) does not show strong patterns, which
supports the use of this model.

\subsubsection{Feature importance and interpretation}

The coefficient plot (bottom left of Figure~\ref{fig:ridge_results})
summarizes the result about which of the statistics matter most for the OPS+:

\begin{itemize}
  \item BABIP has the largest positive coefficient, so batting average
        on balls is the strongest predictor of OPS+ among the
        independent features.
  \item AVG also has a strong positive coefficient, fits the idea
        that making contact and getting hits is important.
  \item K\_pct has a clear negative coefficient, agrees with the
        idea that high strikeout rates hurt overall offensive production.
  \item BB\_pct and BB/K both have positive coefficients, showing the
        value of plate discipline and drawing walks.
  \item PA has a positive relationship with OPS+, because it may
        reflect by both playing time and talent.
\end{itemize}

The distribution comparison (bottom right of Figure~\ref{fig:ridge_results})
shows that the predicted OPS+ values and the actual OPS+ values in the test
set have very similar distributions.
This suggests that the model not only predicts the average level well,
but also captures the spread of OPS+ in this group of hitters.

The Ridge regression results line up well with the logistic regression
results: the same process metrics (contact rate, plate discipline,
balls-in-play outcomes) that separate elite from non-elite hitters
also explain the OPS+ values result.

\section{Discussion}

The robust regression, logistic regression, and Ridge regression analyses
all point to a similar picture of what strong CPBL hitters look like.

In the robust regression analysis, slugging percentage (SLG) stands out
as a key driver of OPS+, which matches basic baseball intuition: power and
extra-base hits matter a lot.
The PCA-based model shows that we can reduce the original batting
statistics to a few components without losing much prediction accuracy,
and that this can make the fits more stable when there are outliers.

The logistic regression analysis uses process-based
statistics that do not directly encode OPS+.
It shows that:
\begin{itemize}
  \item Low strikeout rates (low K\_pct) and high BABIP are both strong
        predictors of being an elite hitter.
  \item Plate appearances (PA) are positively related to elite
        status, which likely reflects both skill and opportunity.
  \item A high BIP\_pct is related to lower odds of being elite,
        possibly because just putting more balls in play without
        good plate discipline or power does not create enough
        valuable outcomes such as walks and extra-base hits.
\end{itemize}

On the test set, the logistic regression model still has high AUC
and good accuracy, so these four process-based features seem to generalize
reasonably well outside the training data.

The Ridge regression model tells the same story on the continuous
OPS+ scale.
It reaches a test $R^2$ close to 0.97 using only independent process-based
features, and again highlights BABIP, AVG, and K\_pct as the main drivers
of OPS+, with positive contributions from walk-related stats (BB\_pct,
BB/K) and PA.

\section{Conclusion and Future Work}

To sum up, this STT 810 project finds that:
\begin{enumerate}
  \item Robust regression with PCA gives a stable and easy-to-read
        model for continuous OPS+, and underlines the important role of
        slugging percentage and other power-related stats.
  \item Logistic regression based on process-based statistics
        (PA, K\_pct, BABIP, BIP\_pct) can separate
        elite from non-elite CPBL hitters, with good test performance
        and high AUC.
  \item Ridge regression using only independent process-based features
        can predict OPS+ very accurately, even without using composite
        outcome measures.
\end{enumerate}

There are a few obvious directions for future work:
\begin{itemize}
  \item Add more seasons and data sources to increase sample size and
        check whether the patterns here are stable over time.
  \item Use more systematic cross-validation for model selection and
        tuning across all model types.
  \item Compare with other regularized and non-linear models
        (for example, Lasso, random forests, gradient boosting)
        as additional benchmarks.
  \item Extend the analysis to position-specific models or to pitcher
        performance metrics.
\end{itemize}

All code, intermediate data, and notebooks for this project are available at:
\begin{center}
  \href{https://github.com/neil7227/STT810-project/tree/main}{\texttt{https://github.com/neil7227/STT810-project/tree/main}}
\end{center}

\newpage

\section*{References}

\begin{thebibliography}{9}

\bibitem{robas}
  Robas (\textit{The Baseball Revolution}) CPBL batter statistics, 2024--2025 seasons.

\bibitem{scikit}
  Scikit-learn developers.
  \textit{Scikit-learn User Guide: Linear and Logistic Regression.}

\bibitem{statsmodels}
  Statsmodels developers.
  \textit{Statsmodels User Guide: Discrete Choice Models.}

\end{thebibliography}

\end{document}
